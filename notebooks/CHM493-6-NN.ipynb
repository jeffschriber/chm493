{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bfdd42-38d2-4756-bc83-dd292550c360",
   "metadata": {},
   "source": [
    "# Neural Networks for Ligand-based Screening\n",
    "\n",
    "*adapted from Volkamer Lab, 2020*\n",
    "\n",
    "## The Problem\n",
    "\n",
    "The epidermal growth factor receptor (EGFR) is a transmembrane protein involved in cell development and in innate immune response in human skin. EGFR requires specific ligands to become activated. Overexpression of EFGR has been linked to numerous types of cancer, where constant activation of EFGRs lead to uncontrolled cell division. \n",
    "\n",
    "Inhibitors of EGFRs have become the target of anticancer therapeutic development. If a drug can bind to overexpressed EFGRs, then the required signal molecules cannot bind EGFR and the signaling cascade resulting in growth/proliferation is stopped. While several drugs, including osimertinib, gefitinib, erlotinib, and bregatinib, have been developed to target EGFRs, patients develop resistance, however. Development of new drugs to target EGFR and its mutations is an active area of drug development.\n",
    "\n",
    "## Our Assignment: *in silico* ligand screening\n",
    "\n",
    "In this notebook, we will build a neural network model to perform ligand-based screening of EGFR inhibitors. There are typically several ways to do this. One approach is to perform molecular dynamics and docking simluations, to computationally determine how well a particular drug candidate binds to a target. This approach is useful, but typically too computationally demanding to apply to thousands of drug candidates. A better first tool is to use a ML-based approach, which can quickly screen through thousands of candidate drugs to produce a small set of primary leads. \n",
    "\n",
    "When building such a model, we have to decide exactly what to predict. A common property to use in drug discovery is the **IC50**. The IC50, or the half-maximal inhibitory concentration, is the concentration of a drug needed to reduce a target biological activity by half. Because these values can span a very wide range, the most common choice in ML models is the **pIC50**, which is the negative log of the IC50. The main difficulty of using pIC50s in ML models is data availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed23381-9927-46ce-937d-56f244099890",
   "metadata": {},
   "source": [
    "## Building Neural Networks\n",
    "\n",
    "To train a NN model, we follow essentially the same steps that we did for other supervised learning models. The main difference is that we now have more hyperparameters to deal with, and we need to more carefully tune how well the model is performing.\n",
    "\n",
    "We will need to use some software not native on ChemCompute. There are two main choices here, and we will use TensorFlow (PyTorch is the other option). Execute the cells below to perform the installation. It may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e47f9e-78dd-4f01-94f7-aa470cc6bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys, Draw, rdFingerprintGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5da04-64c9-4c89-8f03-c2af490b77ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0b26f-0ce1-4330-8900-b17c633dfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51ad45-d2ee-4a17-9d39-a267863697b0",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "We will use experimentally determined IC50s, converted to pIC50s, in our model. These are stored in the file `CHEMBL25_activities_EGFR.csv` in the data folder. In the cell below, load this data into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d149c-e213-49df-8409-df9733d042ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "301cbc80-fab2-411d-bfcb-a58ab981b50e",
   "metadata": {},
   "source": [
    "Now, generate the MACCS fingerprints for all molecules in the dataset. Add these fingerprints as a new column in the dataframe. Print the first few lines of the dataframe to ensure it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37c705-b370-4f99-8bda-721df3f9c6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129e9828-eea7-45bd-af17-a0e8099c5c57",
   "metadata": {},
   "source": [
    "Let's only use these fingerprints as our descriptors, so there's no need for feature selection or normalization. All we need to do next is get our training and testing subsets, do that in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a0ecf-0593-4b98-8507-a524ece3f115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1a1bbc7-e0ce-4f8a-82a0-668c6f03dd1e",
   "metadata": {},
   "source": [
    "## Define and Compile the Model\n",
    "\n",
    "Before actually training the model, we need to define and compile it. This step was a simple one-liner for our regression models, but here we'll need to get into the details a little bit more.\n",
    "\n",
    "Let's define and complile a model with two hidden layers with 64 and 32 nodes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5fb2e-96f4-4e90-91b5-0ecd6f419421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(64, activation=\"relu\", name=\"layer1\"))\n",
    "# Second hidden layer\n",
    "model.add(Dense(32, activation=\"relu\", name=\"layer2\"))\n",
    "# Output layer\n",
    "model.add(Dense(1, activation=\"linear\", name=\"layer3\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376037f-9d23-4f66-91e7-c6cbfc5ae7c0",
   "metadata": {},
   "source": [
    "Note that the output layer has a size of one (for our single-value output), and that the activation fuction goes to linear. Finally, in the cell below, we'll compile the model. This is where we define our loss function and our optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48ec30-6d75-47e4-92fc-49e8ee2eaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\", \"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c0152-4811-4ec5-834d-e44b3c079dc6",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Now we need to train the model. Along with specifying the training features and labels, we need to set a few additional parameters:\n",
    "\n",
    " - Batch Size. Defines the subset of data to use to calculate the gradient in optimizing the loss function.\n",
    " - Epochs. This is the number of times the model will traverse the network. You need a sufficient number of epochs to effectively minimize the loss function, but too many become computationally demanding.\n",
    "\n",
    "Unlike our previous work, we'll need to analyze the performance of the fit. This entails analyzing validation errors as a function of epoch. Complete the function below by filling in the missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1568f5b-4f77-4be8-b048-4859cc4fbf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    , # x values\n",
    "    , # y values\n",
    "    batch_size=, # batch size\n",
    "    validation_data=, # validation data\n",
    "    verbose=0,\n",
    "    epochs=, # number of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b9da2-aade-447a-92d0-ed330f55cd0a",
   "metadata": {},
   "source": [
    "Excecute the cell below to analyze the optimization. You should see a plot of training and validation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359e2de-926c-4256-83b3-0a7ddadb7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"test\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper right\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim((0, 15))\n",
    "plt.title(\n",
    "    f\"test loss = {history.history['val_loss'][nb_epoch-1]:.2f}, \" f\"batch size = {batch}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d1928-a107-44e5-9e26-4ce32563721f",
   "metadata": {},
   "source": [
    "Using the above cells, try to improvy your testing error by testing a few different batch sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70755bbc-a94c-4e5a-8f1f-02b20fa5ff79",
   "metadata": {},
   "source": [
    "We'll want to save the weights from the best performing epoch. This is automated by creating a `ModelCheckpoint`. In the cell below, I've added some code to save the best weights to a file. Complete the fitting function with you data and an optimal batch size you determined earlier, then execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7024df-1cc1-4337-91bc-b730a8b45240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "filepath = \"best_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    str(filepath),\n",
    "    monitor=\"loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode=\"min\",\n",
    "    save_weights_only=True,\n",
    ")\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    , # x values\n",
    "    , # y values\n",
    "    epochs=,\n",
    "    batch_size=,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29581e8-41d5-4575-956e-0c993d73682f",
   "metadata": {},
   "source": [
    "Once you have your final model trained, we can save it to a file, so that we don't need to re-train anything to use it. Excecute the cell beow to save your model. Feel free to give the file a more descriptive name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f1e73-2da0-4b37-9e29-fe73703f98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1165a07-6629-4ad2-8e4b-0a5e874c0055",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "The plots we made above are very useful to analyze our model's performance, but there's more we can do, like calculating other evaluation metrics and visualizing our testing performance.\n",
    "\n",
    "In the cell below, generate a list of predicted pIC50s for your test set. You will want to `flatten` the result, so that the array is the same shape as your testing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2494e-df8f-4980-a262-b5815cb58c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c998322-f8f2-4da5-a1ca-b4ffeb40ec88",
   "metadata": {},
   "source": [
    "Calculate the mean absolute error and the root mean squared error for these predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ce970-a07a-4f8c-9c91-f085908d63f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6ab3b11-2b6e-4d63-a47a-b269bb75aae3",
   "metadata": {},
   "source": [
    "Make a scatter plot of these predictions using matplotlib. Have your predicted values on the $x$-axis, and your true values on the $y$-axis. Also plot the line $y=x$ with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606c043-bec4-4cd1-afb4-2b40a1e1146a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a240272-b44e-494f-9689-f6e7690fa9bc",
   "metadata": {},
   "source": [
    "Use your data to calulate an $R^2$ value for the correlation between your predicted values and the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ee078-4450-4f9a-9bf1-b22154bd9d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
